\maketitle

\begin{abstract}
	In response to the critical need for efficient wildfire detection, this study presents a novel approach leveraging machine learning techniques applied to images from ground cameras and drones. Traditional methods, predominantly relying on satellite imagery, often grapple with false positives and maintenance challenges. Our method aims to address these limitations by utilizing a more direct and responsive source of visual data. The significance of this research is underscored by California's substantial investment in wildfire management, with CalFire allocating \$3.3 billion annually for this purpose. The proposed system, therefore, has the potential to significantly reduce costs and enhance early detection capabilities, particularly in remote areas.
	We curated a comprehensive dataset of 843,862 images, categorized into fire and non-fire classes, from a 16GB image repository sourced from Kaggle. To optimize memory usage and enable efficient batch processing, images were resized to 200Ã—200 pixels. Duplicate removal was achieved through image hashing, and data augmentation techniques expanded our dataset fivefold. This included modifications in zoom, brightness, color jittering, Gaussian noise, and horizontal flipping. All images were standardized to JPEG format.
	For model development, we employed an 80\%-20\% split for training and testing, and an 85\%-15\% split for training and validation. The study experimented with various neural networks, including ResNet, MobileNet, and AlexNet, over 10 epochs using SGD optimization with a momentum of 0.9 and a learning rate of 0.001. ResNet emerged as the most effective model, benefiting from deeper layers and skip connections. MobileNet, while efficient, lacked the complexity needed for pattern recognition, and AlexNet's simpler architecture led to lower performance.
	To enhance the robustness of our model against overfitting, we are considering the implementation of k-fold cross-validation. Additionally, we plan to integrate semantic segmentation for more precise fire localization. Future work will focus on augmenting the dataset with edge case images, particularly those with various light sources, to improve the model's resistance to false positives. This research not only contributes to the field of wildfire detection but also demonstrates the potential of machine learning in addressing environmental and public safety challenges.
\end{abstract}

\section{Introduction}

In the realm of wildfire management and prevention, the development of an advanced early fire detection system stands as a critical innovation. Traditional methods, primarily relying on satellite imagery, are increasingly proving inadequate due to their susceptibility to false positives and ongoing maintenance challenges. To address these limitations, our research introduces a novel approach utilizing Computer Vision (CV) technology, harnessing images from ground-based cameras and drones. This methodology marks a significant leap in accurately identifying the presence of fire, particularly in its nascent stages.

The urgency and importance of this development are underscored by the efforts of organizations such as CalFire. As the state agency responsible for fire protection and the stewardship of over 31 million acres of California's wildlands, CalFire's expenditure of \$3.3 billion for wildfire protection and suppression vividly illustrates the substantial financial and environmental stakes involved. ~\citep{calfire} This immense budget not only highlights the state's commitment to managing and responding to wildfires but also reflects the enormous economic impact of these natural disasters.

In this context, the potential of an early fire detection system cannot be overstated. By enabling quicker and more accurate detection of wildfires, especially in remote or hard-to-reach areas where traditional surveillance methods are limited, such a system could have a profound economic effect. The anticipated benefits include significant cost savings, potentially amounting to millions of dollars annually, and more importantly, the mitigation of extensive environmental and property damage. Our research aims to explore and validate the effectiveness of this CV-based system, positioning it as a pivotal tool in the ongoing battle against wildfires.

\section{Problem Description}

Consectetur aute sit qui laborum Lorem deserunt proident excepteur id ad consequat. Commodo eiusmod fugiat id labore exercitation excepteur. Incididunt deserunt ipsum esse. Cupidatat adipisicing consequat aliqua occaecat dolore esse. Culpa occaecat labore consectetur id ea velit pariatur fugiat ipsum adipisicing quis sit. Quis anim adipisicing commodo sunt aliquip ipsum nulla labore. Ipsum nulla sunt exercitation voluptate laborum proident qui cupidatat pariatur. Eu labore ut deserunt deserunt sint magna id dolor et nulla laborum.

\section{Related Work}


Consectetur aute sit qui laborum Lorem deserunt proident excepteur id ad consequat. Commodo eiusmod fugiat id labore exercitation excepteur. Incididunt deserunt ipsum esse. Cupidatat adipisicing consequat aliqua occaecat dolore esse. Culpa occaecat labore consectetur id ea velit pariatur fugiat ipsum adipisicing quis sit. Quis anim adipisicing commodo sunt aliquip ipsum nulla labore. Ipsum nulla sunt exercitation voluptate laborum proident qui cupidatat pariatur. Eu labore ut deserunt deserunt sint magna id dolor et nulla laborum.

\section{Data Collection}

Commodo velit magna Lorem reprehenderit excepteur consequat mollit consequat nisi. Adipisicing nostrud ullamco nostrud commodo ea aliqua. Laboris ut do officia id ipsum commodo. Nostrud aute consectetur sint excepteur veniam. Incididunt fugiat magna Lorem occaecat cillum. Quis exercitation Lorem occaecat reprehenderit eiusmod proident laborum anim minim nisi est. Magna tempor excepteur aliquip ut commodo do laboris adipisicing in laboris veniam voluptate fugiat nisi eu.

\section{Preprocessing}

Occaecat est ad non voluptate occaecat labore elit voluptate officia ipsum Lorem deserunt dolore labore voluptate. Anim eu ut voluptate quis. Duis adipisicing excepteur ut occaecat laborum ullamco ad laborum do consectetur laboris in ea culpa. Ad pariatur nisi quis eiusmod ea culpa irure incididunt duis laborum.

\section{Model and Methods}

Adipisicing culpa id reprehenderit. Excepteur pariatur quis tempor id cupidatat laborum sunt id reprehenderit deserunt. Culpa mollit exercitation id officia enim deserunt culpa nisi Lorem ullamco tempor ut culpa laboris. Officia nulla sint excepteur. Ea non nostrud tempor voluptate excepteur consequat culpa tempor dolore laboris eiusmod consequat incididunt. Minim enim labore commodo. Enim dolore amet officia consectetur quis elit proident duis labore cupidatat.

\section{Results}

Occaecat sint ad aliquip ut ipsum officia eu mollit cillum. Ullamco eu cupidatat deserunt deserunt et ut pariatur esse non Lorem ut nisi labore proident. Voluptate magna enim laboris adipisicing elit mollit qui Lorem mollit nostrud ipsum magna eiusmod aliquip. Quis velit et adipisicing adipisicing.

\section{Future Directions}

Excepteur nulla eu et qui consequat quis sunt reprehenderit velit quis eiusmod non. Nostrud consequat esse quis fugiat Lorem officia minim elit esse do anim elit laborum ullamco. Aute nisi nulla officia nisi Lorem tempor enim voluptate cupidatat est. Deserunt ipsum veniam aute id sunt.

\section{Conclusions}

Dolore nulla ex duis sint mollit amet deserunt dolor laborum. Ex occaecat do minim fugiat ea laboris incididunt est cillum minim sint incididunt fugiat deserunt. Adipisicing adipisicing commodo laborum consectetur dolor cillum Lorem sint deserunt irure proident qui. Veniam aute officia voluptate sunt qui ex. Sunt ipsum irure elit consectetur. Nulla ullamco eiusmod ullamco mollit. Incididunt cillum in id ad id sint qui amet. Do aute sit aute ipsum est ut reprehenderit voluptate exercitation exercitation.

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone, regardless of the formatter being used.

\subsection{Citations within the text}

Citations within the text should be based on the \texttt{natbib} package
and include the authors' last names and year (with the ``et~al.'' construct
for more than two authors). When the authors or the publication are
included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
towards AI~\citep{Bengio+chapter2007}.'').

The corresponding references are to be listed in alphabetical order of
authors, in the \textsc{References} section. As to the format of the
references themselves, any style is acceptable as long as it is used
consistently.

\subsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first footnote} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches
(12~picas).\footnote{Sample of the second footnote}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction; art work should not be
hand-drawn. The figure number and caption always appear after the
figure. Place one line space before the figure caption, and one line
space after the figure. The figure caption is lower case (except for
first word and proper nouns); figures are numbered consecutively.

Make sure the figure caption does not get separated from the figure.
Leave sufficient space to avoid splitting the figure and figure caption.

You may use color figures.
However, it is best for the
figure captions and the paper body to make sense if the paper is printed
either in black/white or in color.
\begin{figure}[h]
	\begin{center}
		%\framebox[4.0in]{$\;$}
		\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
	\end{center}
	\caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
	\caption{Sample table title}
	\label{sample-table}
	\begin{center}
		\begin{tabular}{ll}
			\multicolumn{1}{c}{\bf PART} & \multicolumn{1}{c}{\bf DESCRIPTION}
			\\ \hline \\
			Dendrite                     & Input terminal                      \\
			Axon                         & Output terminal                     \\
			Soma                         & Cell body (contains cell nucleus)   \\
		\end{tabular}
	\end{center}
\end{table}

\subsubsection*{Author Contributions}
If you'd like to, you may include  a section for author contributions as is done
in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}
